# **HormAS: A Framework for Generating Empathetic and Strategic Responses to Customer Reviews**
This repository contains the source code and documentation for **HormAS (Hotel Review Management Assistant System)**, a framework designed to generate nuanced and effective responses to customer reviews. HormAS leverages a multi-stage process involving sentiment, emotion, and intention analysis to craft replies that are not only contextually appropriate but also strategically aligned with reputation management goals.

***

## **1. System Architecture and Implementation**

In response to feedback requesting greater transparency, this section provides a detailed overview of the HormAS framework, encompassing its frontend, backend, and database components. The entire system is designed based on the Design Science Research (DSR) methodology to ensure relevance and rigor.

<br>
<p align="center">
  <img src="/pic/pic1.png" width="800">
  <br>
  <em>Figure 1: The HormAS System Architecture and its mapping to the Design Science Research cycles.</em>
</p>
<br>

### **Backend**
The backend is developed using **Python 3.10.2** with the **Django 5.0.1** framework to ensure stability and scalability. Its core responsibilities include:
* **NLP and Machine Learning:** Core logic for prompt processing and response generation is implemented using **LangChain 0.1.3**. This server leverages powerful Large Language Models like **OpenAI's GPT-4** and models from the **Hugging Face** ecosystem to perform analysis and generation tasks.
* **API Service:** **Django** application serves the processed outputs through a RESTful API, which is managed and exposed via **Nginx**. This server handles data aggregation and requests between the frontend and the machine learning components.

### **Frontend**
The frontend provides an interactive web interface for users. It was built with **Next.js 15.3.2 (React 19.0.0)** and served via **Node.js**. This interface allows users to:
* Input customer review texts.
* Initiate the analysis and generation process.
* Compare responses generated by the HormAS framework against those from general-purpose language models.

### **Database and Infrastructure**
* **Database:** The system employs **Amazon RDS (PostgreSQL 15)** to store user inputs, the extracted analytical data (sentiment, emotion, intention), and the final generated responses for evaluation and further analysis. A backup mechanism to AWS is in place for data integrity.
* **Infrastructure:** The entire framework is containerized using **Docker** and orchestrated with **Kubernetes**, allowing for scalable deployment and robust management of the microservices architecture.

***

## **2. Core Methodologies: Prompt Engineering and Evaluation**

HormAS utilizes a sophisticated generating framework that first deconstructs user input and then synthesizes that analysis into a final response. This process is orchestrated through advanced prompt engineering techniques.

<br>
<p align="center">
  <img src="/pic/pic2.png" width="800">
  <br>
  <em>Figure 2: The HormAS Generating Framework.</em>
</p>
<br>


### **2.1. A Guide to Prompt Engineering: CoT, Zero-shot CoT, and Function Calling**

This document explains and provides concrete examples for key prompt engineering techniques that maximize the reasoning and data extraction abilities of Large Language Models (LLMs): **Chain-of-Thought (CoT)**, **Zero-shot CoT**, and **Function Calling**.

***


#### **2.1.1. Chain-of-Thought (CoT) Prompting**

**Concept**
CoT is a technique that guides an LLM to solve complex problems step-by-step by providing examples that include the **reasoning process or chain of thought** leading to the answer, rather than just the final answer itself. This approach significantly improves LLM performance, especially for tasks requiring arithmetic, commonsense, or symbolic reasoning.

For creative or analytical tasks, CoT can also be used by **instructing the model to follow a specific reasoning framework** before producing the final output. The model generates its own Chain of Thought based on these instructions.

**Key Paper**
* **Title:** [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
* **Authors:** Jason Wei, Xuezhi Wang, Dale Schuurmans, et al. (2022)

**Example: Generating a Hotel Review Response**
In this example, the prompt instructs the model to perform a multi-step analysis *before* writing the final response to a customer.

**Hyper-Parameter**
* **Temperature:** 0.1
* **Top-P:** 1.0

**✅ CoT Prompt Template**

```
You are an expert Marketing Manager specializing in online reputation management. Your task is to craft a thoughtful and effective response to a customer review.

To ensure the highest quality response, you must follow this step-by-step reasoning process before writing the final reply.

## Step-by-Step Analysis ##

Step 1: Analyze Customer Sentiment.

Based on the provided 'Customer Sentiment', briefly explain how the overall tone of your response (e.g., apologetic, grateful, neutral) should be shaped.

Step 2: Analyze Customer Emotion.

Based on the 'Customer Emotion', identify the core feeling the customer is expressing (e.g., frustration, delight, disappointment). How will you validate this specific emotion in your response?

Step 3: Analyze Customer Intention.

Based on the 'Customer Intention', determine what the customer hopes to achieve with their review (e.g., receive a solution, warn others, praise the service). What specific action or information must your response contain to address this intention?

Step 4: Synthesize and Formulate a Strategy.

Based on your analysis in steps 1-3, outline a brief strategy for the final response. Mention the key points you will include.

## Final Response Generation ##

After completing the analysis above, write the final, polished response to the customer. This response should directly reflect the insights from your step-by-step analysis.

Final Response to Customer:
[Your final, well-crafted response goes here]
```

***

#### **2.1.2. Zero-shot CoT Prompting**

**Concept**
Zero-shot CoT is a highly efficient technique that retains the power of CoT **without the need to provide any examples**. It works by simply appending a simple phrase, such as **"Let's think step by step,"** to the end of the prompt, which guides the model to generate its own reasoning process before arriving at an answer.

**Key Paper**
* **Title:** [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)
* **Authors:** Takeshi Kojima, Shiorie Shimoeda, Machel Reid, et al. (2022)

**Example: Generating a Hotel Review Response**
In this scenario, the model is given pre-analyzed data and is simply prompted to "think step by step" to synthesize this information into a final response.

**Hyper-Parameter**
* **Temperature:** 0.1
* **Top-P:** 1.0

**✅ Zero-shot CoT Prompt**
```
You are a professional Marketing Manager responsible for handling customer feedback. Your goal is to write a personalized and appropriate response to the following customer review, taking into account the provided analysis of their sentiment, emotion, and intention.

Carefully review all the information provided below.

Your final output should be only the direct response to the customer.

Let's think step by step.
```
***

#### **2.1.3. Function Calling for Structured Output**

**Concept**
Function Calling is a technique that allows developers to get **structured data outputs** from the model. Instead of generating free-form text, the LLM can be instructed to generate a JSON object containing arguments to "call" a function defined by the user. This is exceptionally powerful for reliable data extraction, classification, and integrating LLMs with external tools or APIs. This is the core mechanism used in the first stage of the HormAS framework to extract Sentiment, Emotion, and Intention.

<br>
<p align="center">
  <img src="/pic/pic5.png" width="800">
  <br>
  <em>Figure 3: Comparison of a generic multi-stage prompt with the HormAS extraction process.</em>
</p>
<br>

### How it Works
The process typically involves:
1.  Defining a function's signature (its name, description, and parameters) in the request to the LLM.
2.  Providing a user prompt (e.g., a customer review to be analyzed).
3.  The LLM analyzes the prompt and, if it decides to "call" the function, it generates a structured JSON object with the appropriate arguments based on the schema you provided.
4.  Your application can then parse this JSON to perform further actions, like updating a database or calling another API.

### Example: Analyzing a Hotel Review with Function Calling
To reliably extract sentiment, emotion, and intention from a customer review, we can define a function schema and ask the LLM to call it.

**Hyper-Parameter**
* **Temperature:** 0.0
* **Top-P:** 0.8

#### ✅ Function Definition (Schema)
This is the function schema provided to the LLM. It defines the structure of the desired output.
```json
[
    {
        "name": "Describer",
        "description": "Analyze the following 'Review'",
        "parameters": {
            "type": "object",
            "properties": {
                "User_Sentiment": {
                    "type": "string",
                    "enum": ["Positive", "Neutral", "Negative"],
                    "description": "Sentiment Analysis of the 'Review'"
                },
                "User_Emotion": {
                    "type": "string",
                    "enum": ["Anger", "Disgust", "Fear", "Happiness", "Contempt", "Sadness", "Surprise"],
                    "description": "Emotion Analysis of the 'Review'"
                },
                "User_Intention": {
                    "type": "string",
                    "description": "Intention Analysis of the 'Review'"
                }
            },
            "required": ["User_Sentiment", "User_Emotion", "User_Intention"]
        }
    }
]
```

#### ✅ Sample Prompt & Expected Output

**User Prompt to LLM:**
```
Please analyze the following customer review:

"I waited 30 minutes for a cold coffee. The barista was also very rude. I'm never coming back."
```

**Expected LLM Output (A JSON object to "call" the function):**
The LLM doesn't respond in prose, but rather with the structured data required to call the `Describer` function.
```json
{
  "name": "Describer",
  "arguments": {
    "User_Sentiment": "Negative",
    "User_Emotion": "Anger",
    "User_Intention": "Complain"
  }
}
```
***
## **3. Embedding Similarity Analysis**

This section analyzes the semantic similarity between a human-written response and an AI-generated response using embedding vectors and Cosine Similarity.

### **Response A (Human-Written)**
> We are currently investigating the technical glitch you experienced to determine if it was an isolated incident. We sincerely apologize for the inconvenience this caused and would like to discuss further what we can do to regain your trust. Please reach out to us via email at your earliest convenience. We look forward to hearing from you.

### **Response B (Generated by AI - Zero-shot CoT Prompt)**
> Dear Valued Customer,
>
> First and foremost, we want to extend our sincerest apologies for the difficulties you experienced due to the issue with our website's booking system. We understand this situation has caused you significant trouble, and it is clear that our team's lack of communication and empathy at a time when you needed assistance must have been deeply distressing. This falls far short of the high standard of service we aim to provide, and we are truly sorry for letting you down.
>
> Please know that we are taking your feedback very seriously. We are currently investigating the bug in our booking system to ensure this does not happen to other guests. Additionally, we are reviewing our customer service protocols to improve how we handle such situations in the future, ensuring our team members offer the proper apologies and support during customer difficulties.
>
> While we understand that nothing can change the fact that you were left without accommodation at the last minute, we want to make this right. If you are willing, please contact us directly at [Insert Contact Information] and ask for [Insert Manager's Name], and we will be glad to discuss how we can resolve this situation to your satisfaction. This could include arranging for alternative accommodation or offering compensation for the inconvenience caused.
>
> Once again, we apologize for the stress and inconvenience this incident has caused you. We value your feedback as it helps us improve, and we hope to have the opportunity to restore your faith in our service.
>
> Sincerely,

#### **Analysis and Interpretation**

The Cosine Similarity score between the embeddings of the two responses was calculated to quantify their semantic relationship.

**Similarity Score: `0.5458`**

**Interpretation:**
Cosine Similarity measures the directional similarity between two vectors, yielding a value between -1 (opposite) and 1 (identical). A score approaching 1 indicates a high degree of semantic similarity, whereas a score near 0 suggests a lack of correlation.

#### **3.1 Overall Comparison of Prompting Methods**

To evaluate the overall effectiveness of different prompting strategies, we calculated the average Cosine Similarity score between human-written reference responses and the AI-generated responses for each method across our test dataset.

| Prompt Method | Average Cosine Similarity Score |
| :--- | :--- |
| **Basic Prompt** | `0.5560` |
| **CoT Prompt** | `0.5469` |
| **Zero-shot CoT Prompt** | ***`0.5636`*** |
***

## 4. References
* Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., & Zhou, D. (2022). *Chain-of-Thought Prompting Elicits Reasoning in Large Language Models*. ArXiv. /abs/2201.11903
* Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). *Large Language Models are Zero-Shot Reasoners*. ArXiv. /abs/2205.11916
* (Note: Function Calling was popularized by OpenAI. See the [OpenAI API documentation](https://platform.openai.com/docs/guides/function-calling) for their implementation.)
